{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img-blog.csdn.net/2018092916042238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thaWVuMTIyNg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\" style=\"width:1000px;height:500px;float:left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 6>\n",
    "  tf.Session \n",
    "</font>\n",
    "\n",
    "<hr class=\"docutils\" />\n",
    "\n",
    "<blockquote><p><font size= 4>\n",
    "Defined in tensorflow/python/client/session.py.\n",
    "<p>\n",
    "A class for running TensorFlow operations.\n",
    "<p>\n",
    "A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. For example:\n",
    "    </font>\n",
    "</blockquote>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> Example  </font></th>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Build a graph.\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "\n",
    "# Evaluate the tensor `c`.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><p><font size= 4>\n",
    "A session may own resources, such as tf.Variable, tf.QueueBase, and tf.ReaderBase. It is important to release these resources when they are no longer required. To do this, either invoke the tf.Session.close method on the session, or use the session as a context manager. The following two examples are equivalent:\n",
    "<p>\n",
    "    <code>\n",
    "    # Using the `close()` method.\n",
    "    sess = tf.Session()\n",
    "    sess.run(...)\n",
    "    sess.close()\n",
    "\n",
    "    # Using the context manager.\n",
    "    with tf.Session() as sess:\n",
    "    sess.run(...)\n",
    "  </code>\n",
    "<p>\n",
    "The ConfigProto protocol buffer exposes various configuration options for a session. For example, to create a session that uses soft constraints for device placement, and log the resulting placement decisions, create a session as follows:\n",
    "    <code>\n",
    "    # Launch the graph in a session that allows soft device placement and\n",
    "    # logs the placement decisions.\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                            log_device_placement=True))\n",
    "    </code>\n",
    "    \n",
    "   </font>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 4>\n",
    "  Supported devices\n",
    "</font>\n",
    "\n",
    "<hr class=\"docutils\" />\n",
    "<blockquote><p><font size= 4>\n",
    "On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are CPU and GPU. They are represented as strings. For example:\n",
    "<p>\n",
    "    \"/cpu:0\": The CPU of your machine.\n",
    "<p>\n",
    "    \"/device:GPU:0\": The GPU of your machine, if you have one.\n",
    "<p>\n",
    "    \"/device:GPU:1\": The second GPU of your machine, etc.\n",
    "<p>\n",
    "If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device. For example, matmul has both CPU and GPU kernels. On a system with devices cpu:0 and gpu:0, gpu:0 will be selected to run matmul.\n",
    "    </font>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 4>\n",
    "  Logging Device placement\n",
    "</font>\n",
    "\n",
    "<hr class=\"docutils\" />\n",
    "<blockquote><p><font size= 4>\n",
    "To find out which devices your operations and tensors are assigned to, create the session with log_device_placement configuration option set to True.\n",
    "    </font>\n",
    "</blockquote>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> Example  </font></th>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 4>\n",
    "  Manual device placement\n",
    "</font>\n",
    "\n",
    "<hr class=\"docutils\" />\n",
    "<blockquote><p><font size= 4>\n",
    "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with tf.device to create a device context such that all the operations within that context will have the same device assignment.\n",
    "    </font>\n",
    "</blockquote>\n",
    "\n",
    "<table style=\"width:100%;border-collapse:collapse;background-color:white;\">\n",
    "    <tr style=\"background-color:rgba(30, 200,100, 0.8)\" >\n",
    "      <th> <font size= 5> Example  </font></th>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote><p><font size= 4>\n",
    "You will see that now a and b are assigned to cpu:0. Since a device was not explicitly specified for the MatMul operation, the TensorFlow runtime will choose one based on the operation and available devices (gpu:0 in this example) and automatically copy tensors between devices if required.\n",
    "    </font>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=#8552a1 size= 4>\n",
    "  Allowing GPU memory growth\n",
    "</font>\n",
    "\n",
    "<hr class=\"docutils\" />\n",
    "<blockquote><p><font size= 4>\n",
    "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use with tf.device to create a device context such that all the operations within that context will have the same device assignment.\n",
    "<p>\n",
    "By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process. This is done to more efficiently use the relatively precious GPU memory resources on the devices by reducing memory fragmentation.\n",
    "<p>\n",
    "In some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. TensorFlow provides two Config options on the Session to control this.\n",
    "<p>\n",
    "The first is the allow_growth option, which attempts to allocate only as much GPU memory based on runtime allocations: it starts out allocating very little memory, and as Sessions get run and more GPU memory is needed, we extend the GPU memory region needed by the TensorFlow process. Note that we do not release memory, since that can lead to even worse memory fragmentation. To turn this option on, set the option in the ConfigProto by:\n",
    "<code>\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config, ...)\n",
    "</code>\n",
    "<p>\n",
    "The second method is the per_process_gpu_memory_fraction option, which determines the fraction of the overall amount of memory that each visible GPU should be allocated. For example, you can tell TensorFlow to only allocate 40% of the total memory of each GPU by:\n",
    "<code>\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config, ...)\n",
    "    </code>\n",
    "    <p>\n",
    "This is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process.\n",
    "</font>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
